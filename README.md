# Generative Modelling

<img width="995" height="395" alt="image" src="https://github.com/user-attachments/assets/a568cc5d-f162-446f-8eda-60c84285eed5" />

A notebook series introducing the theory and implementation of generative models, from classical latent variable models to modern deep architectures. Each notebook contains mathematical derivations and from-scratch implementations.

## ðŸ“‚ Repository Structure

### 1. Latent Variable Models
- [1.1 Mixture Models](./1.%20Latent%20Variable%20Models/1.1%20Mixture%20Models.ipynb/) - model data as a mixture of distributions, introducing discrete latent variables to capture clusters.
- [1.2 Factor Analysis](./1.%20Latent%20Variable%20Models/1.2%20Factor%20Analysis.ipynb) - explain data as linear combinations of continuous latent factors, uncovering hidden structure behind observed correlations.
- [1.3 Variational Autoencoders (VAEs)](./1.%20Latent%20Variable%20Models/1.3%20Variational%20Autoencoders.ipynb) - extend to non-linear mappings with neural networks, using continuous latent variables and variational inference.

### 2. Flow-based Models
- 2.1 Discrete Flow Models
- 2.2 Continuous Flow Models

### 3. Implicit Models
- 3.1 Generative Adversarial Networks (GANs)
- 3.2 Score-Based Models

### 4. Diffusion Models
*Coming soon*

### 5. Autoregressive Models
*Coming soon*

## ðŸ“š References
This series loosely follows the structure of *Deep Generative Modeling* by Jakub M. Tomczak (Springer, 2022) alongside key seminal papers.
